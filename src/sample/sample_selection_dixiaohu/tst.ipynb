{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_config: \n",
      "{\n",
      "    \"hive_uris\": \"thrift://172.22.20.137:7004,thrift://172.22.20.110:7004\",\n",
      "    \"java_home\": \"/opt/jdk1.8.0_181-cloudera\",\n",
      "    \"kubernetes_namespace\": \"kubeflow-user-example-com\",\n",
      "    \"kubernetes_runtime_image\": \"digit-force-docker.pkg.coding.net/ai-platform/base-images/spark-k8s-runtime-with-jars:2.4.8\",\n",
      "    \"master_uri\": \"k8s://https://172.22.20.16\",\n",
      "    \"spark_home\": \"/opt/spark-2.4.8-bin-hadoop2.7\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "23/03/08 02:11:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from digitforce.aip.common.utils.spark_helper import SparkClient\n",
    "import findspark\n",
    "findspark.init()\n",
    "# 需要固定位置的import 后添加 # NOQA: E402， 表示忽略模块级别导入不在文件顶部的错误\n",
    "from pyspark.sql import window as W  # NOQA: E402\n",
    "from pyspark.sql import functions as F  # NOQA: E402\n",
    "from pyspark.sql import types as T  # NOQA: E402\n",
    "from utils import *  # NOQA: E402\n",
    "from digitforce.aip.common.utils.time_helper import DATE_FORMAT\n",
    "spark_client = SparkClient.get()\n",
    "spark = spark_client.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/08 02:11:40 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from sample_select import *\n",
    "dixiao_before_days = 3\n",
    "dixiao_after_days = 3\n",
    "right_zc_threshold = 100\n",
    "avg_zc_threshold = 100\n",
    "event_tag = 'login'\n",
    "sample_table_name = start_sample_selection(\n",
    "        dixiao_before_days=dixiao_before_days,\n",
    "        dixiao_after_days=dixiao_after_days,\n",
    "        right_zc_threshold=right_zc_threshold,\n",
    "        avg_zc_threshold=avg_zc_threshold,\n",
    "        event_tag=event_tag,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dixiao_before_days = 10\n",
    "dixiao_after_days = 10\n",
    "right_zc_threshold=10000\n",
    "avg_zc_threshold=10000\n",
    "feature_days = 10\n",
    "event_tag = 'login'\n",
    "# dixiao_before_days = max(dixiao_act_before_days,dixiao_zc_before_days)\n",
    "# # dixiao_after_days = dixiao_after_days\n",
    "# # right_zc_threshold=10000\n",
    "# # avg_zc_threshold=10000\n",
    "# feature_days = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_FORMAT = \"%Y%m%d\"\n",
    "user_table = \"zq_standard.dm_cust_label_base_attributes_df\"\n",
    "app_table = \"zq_standard.dm_cust_traf_behv_aggregate_df\"\n",
    "zj_table = \"zq_standard.dm_cust_capital_flow_aggregate_df\"\n",
    "jy_table = \"zq_standard.dm_cust_subs_redm_event_aggregate_df\"\n",
    "zc_table = \"zq_standard.dm_cust_ast_redm_event_df\"\n",
    "user_view = user_table[user_table.find(\".\")+1:]\n",
    "app_view = app_table[app_table.find(\".\")+1:]\n",
    "zj_view = zj_table[zj_table.find(\".\")+1:]\n",
    "jy_view = jy_table[jy_table.find(\".\")+1:]\n",
    "zc_view = zc_table[zc_table.find(\".\")+1:]\n",
    "\n",
    "\n",
    "# 1.获取关键时间点\n",
    "window_test_days = 3\n",
    "window_train_days = 5\n",
    "now = datetime.datetime.now()\n",
    "dixiao_end_date = now - datetime.timedelta(days=2)  # 低效户结束日期\n",
    "end_date = dixiao_end_date - \\\n",
    "    datetime.timedelta(days=dixiao_after_days)  # 低效户结束日期\n",
    "mid_date = end_date - datetime.timedelta(days=window_test_days)\n",
    "start_date = mid_date - datetime.timedelta(days=window_train_days)\n",
    "dixiao_start_date = start_date - datetime.timedelta(\n",
    "    days=dixiao_before_days\n",
    ")  # 低效户开始日期\n",
    "feature_date = dixiao_start_date - \\\n",
    "    datetime.timedelta(days=feature_days)  # 特征数据最早日期\n",
    "\n",
    "now = now.strftime(DATE_FORMAT)\n",
    "dixiao_end_date = dixiao_end_date.strftime(DATE_FORMAT)\n",
    "end_date = end_date.strftime(DATE_FORMAT)\n",
    "mid_date = mid_date.strftime(DATE_FORMAT)\n",
    "start_date = start_date.strftime(DATE_FORMAT)\n",
    "dixiao_start_date = dixiao_start_date.strftime(DATE_FORMAT)\n",
    "feature_date = feature_date.strftime(DATE_FORMAT)\n",
    "# 2. 特征预处理\n",
    "spark_client.get_starrocks_table_df(\n",
    "    user_table).createOrReplaceTempView(user_view)\n",
    "spark_client.get_starrocks_table_df(\n",
    "    app_table).createOrReplaceTempView(app_view)\n",
    "spark_client.get_starrocks_table_df(\n",
    "    zj_table).createOrReplaceTempView(zj_view)\n",
    "spark_client.get_starrocks_table_df(\n",
    "    jy_table).createOrReplaceTempView(jy_view)\n",
    "spark_client.get_starrocks_table_df(\n",
    "    zc_table).createOrReplaceTempView(zc_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sample_select import *\n",
    "# 非交易日列表\n",
    "noexchangedate_list = noexchange_days(\n",
    "    start_date=datetime.datetime.strptime(\n",
    "        dixiao_start_date, '%Y%m%d').strftime('%Y-%m-%d'),\n",
    "    end_date=datetime.datetime.strptime(\n",
    "        dixiao_end_date, '%Y%m%d').strftime('%Y-%m-%d'),\n",
    ")\n",
    "# 资产数据\n",
    "zzc_sample_rdd = get_zc_sample(\n",
    "    spark,\n",
    "    zc='total_ast',\n",
    "    zc_view=zc_view,\n",
    "    dixiao_start_date=dixiao_start_date,\n",
    "    dixiao_end_date=dixiao_end_date,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    dixiao_before_days=dixiao_before_days,\n",
    "    dixiao_after_days=dixiao_after_days,\n",
    "    noexchangedate_list=noexchangedate_list\n",
    ")\n",
    "zzc_sample = (\n",
    "    zzc_sample_rdd.toDF(\n",
    "        [\"cust_code\", \"right_zzc\", \"past_avg_zzc\", \"future_max_zzc\", \"dt\"])\n",
    ")\n",
    "zc_sample_df = zzc_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zc_sample_df.select('dt').distinct().orderBy(F.desc('dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "login_sample = (\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "            select cust_code,is_login,replace(dt,'-','') as dt\n",
    "            from {app_view} \n",
    "            where replace(dt,'-','') between '{dixiao_start_date}' and '{dixiao_end_date}'\n",
    "            \"\"\"\n",
    "    )\n",
    "    .rdd\n",
    "    .map(lambda x: (x[0], [(x[2], int(x[1]))]))\n",
    "    .reduceByKey(lambda a, b: a+b)\n",
    "    # 每个x[0] 下面按照日期排序\n",
    "    .map(lambda x: (x[0], sorted(x[1], key=lambda y: int(y[0]), reverse=False)))\n",
    "    # 得到当前日期，过去登陆天数\n",
    "    .map(lambda x: (x[0], get_login_days(x[1], start_date, end_date, dixiao_before_days, noexchangedate_list)))\n",
    "    .flatMapValues(lambda x: x)\n",
    "    .map(lambda x: (x[0], x[1][1], x[1][0]))\n",
    ")\n",
    "\n",
    "login_sample_df = login_sample.toDF(\n",
    "    [\"cust_code\", \"before_login_days\", \"dt\"])\n",
    "\n",
    "# 交易数据\n",
    "exchange_sample = (\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "            select cust_code,total_tran_cnt,replace(dt,'-','') as dt\n",
    "            from {jy_view} \n",
    "            where replace(dt,'-','') between '{dixiao_start_date}' and '{dixiao_end_date}'\n",
    "            \"\"\"\n",
    "    )\n",
    "    .rdd\n",
    "    .map(lambda x: (x[0], [(x[2], int(x[1]))]))\n",
    "    # 每个x[0] 下面按照日期排序\n",
    "    .map(lambda x: (x[0], sorted(x[1], key=lambda y: int(y[0]), reverse=False)))\n",
    "    .reduceByKey(lambda a, b: a+b)\n",
    "    # 得到当前日期，过去登陆天数\n",
    "    .map(lambda x: (x[0], get_exchange_days(x[1], start_date, end_date, dixiao_before_days, noexchangedate_list)))\n",
    "    .flatMapValues(lambda x: x)\n",
    "    .map(lambda x: (x[0], x[1][1], x[1][0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "_schema1 = T.StructType([\n",
    "    T.StructField('cust_code', T.StringType(),True),\n",
    "    T.StructField('before_exchange_days',T.LongType(),True),\n",
    "    T.StructField('dt',T.StringType(),True)\n",
    "])\n",
    "exchange_sample_df = spark.createDataFrame(spark.sparkContext.emptyRDD(),_schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_sample_df.rdd.isEmpty()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exchange_sample_df = exchange_sample.toDF([\"cust_code\", \"before_exchange_days\", \"dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_df = (\n",
    "    zc_sample_df\n",
    "    .join(login_sample_df, on = ['cust_code','dt'],how='left')\n",
    "    .join(exchange_sample_df, on = ['cust_code','dt'],how='left')\n",
    "    .filter(\n",
    "        F.expr(\n",
    "            f\"\"\"\n",
    "            right_zzc<{right_zc_threshold} and right_zzc>0 and past_avg_zzc<{avg_zc_threshold}\n",
    "            \"\"\"\n",
    "        )\n",
    "    ) # 过滤时点资产和平均资产\n",
    ")\n",
    "if event_tag == 'login':\n",
    "    sample_df = sample_df.filter(F.expr(f\"before_login_days>0\"))\n",
    "elif event_tag == 'exchange':\n",
    "    sample_df = sample_df.filter(F.expr(f\"before_exchange_days>0\"))\n",
    "    \n",
    "sample_df = (\n",
    "    sample_df.withColumn(\n",
    "        \"label\",\n",
    "        F.expr(f\"IF(future_max_zzc>{right_zc_threshold},'1','0')\")\n",
    "    )\n",
    "    .select(\"cust_code\", \"label\", \"dt\")\n",
    ")\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hive(spark, inp_df, table_name, partition_col):\n",
    "    check_table = (\n",
    "        spark._jsparkSession.catalog().tableExists(table_name)\n",
    "    )\n",
    "\n",
    "    if check_table:  # 如果存在该表\n",
    "        print(\"table:{} exist......\".format(table_name))\n",
    "        inp_df.write.format(\"orc\").mode(\"overwrite\").insertInto(table_name)\n",
    "\n",
    "    else:  # 如果不存在\n",
    "        print(\"table:{} not exist......\".format(table_name))\n",
    "        inp_df.write.format(\"orc\").mode(\"overwrite\").partitionBy(\n",
    "            partition_col\n",
    "        ).saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sample_select import write_hive\n",
    "sample_table_name = \"algorithm.aip_zq_dixiaohu_custom_label\"\n",
    "write_hive(spark, sample_df, sample_table_name, \"dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 客户号，年龄，性别，城市，省份，教育程度(end_date 的基础信息)\n",
    "table_user = spark.sql(\n",
    "    f\"\"\"\n",
    "    select cust_code, age, gender, city_name, province, educational_degree from {user_view} where replace(dt,'-','') = '{end_date}'\n",
    "    \"\"\"\n",
    ")\n",
    "# 客户号，日期，客户是否登录\n",
    "table_app = spark.sql(\n",
    "    f\"\"\"\n",
    "    select cust_code, replace(dt,'-','') as dt, is_login from {app_view} where replace(dt,'-','') between '{feature_date}' and '{end_date}'\n",
    "    \"\"\"\n",
    ")\n",
    "# 客户号，日期，资金转出金额，资金转入金额，资金转出笔数，资金转入笔数\n",
    "table_zj = spark.sql(\n",
    "    f\"\"\"\n",
    "    select cust_code, replace(dt,'-','') as dt, zc_money, zr_money, zc_cnt, zr_cnt \n",
    "    from {zj_view} \n",
    "    where replace(dt,'-','') between '{feature_date}' and '{end_date}'\n",
    "    \"\"\"\n",
    ")\n",
    "# 客户号，日期，交易笔数，交易金额，股票笔数，股票金额，基金笔数，基金金额\n",
    "table_jy = spark.sql(\n",
    "    f\"\"\"\n",
    "    select cust_code, replace(dt,'-','') as dt, jy_num, jy_rmb, jygp_num, jygp_rmb, jyjj_num, jyjj_rmb \n",
    "    from {jy_view} \n",
    "    where replace(dt,'-','') between '{feature_date}' and '{end_date}'\n",
    "    \"\"\"\n",
    ")\n",
    "# 客户号，日期，总资产，总负债，基金资产，股票资产，资金余额，产品资产\n",
    "table_zc = spark.sql(\n",
    "    f\"\"\"\n",
    "    select cust_code, replace(dt,'-','') as dt, ast_total, ast_fz, ast_jj, ast_gp, ast_zj, ast_cp \n",
    "    from {zc_view} \n",
    "    where replace(dt,'-','') between '{feature_date}' and '{end_date}'\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# 3. 特征融合\n",
    "data = (\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT cust_code,label,dt\n",
    "        FROM {sample_table_name}\n",
    "        WHERE dt>= {dixiao_start_date} and dt <= {end_date}\n",
    "        \"\"\"\n",
    "    )\n",
    "    .join(table_user, on=[\"cust_code\"], how=\"left\")\n",
    "    .join(table_app, on=[\"cust_code\", \"dt\"], how=\"left\")\n",
    "    .join(table_zj, on=[\"cust_code\", \"dt\"], how=\"left\")\n",
    "    .join(table_jy, on=[\"cust_code\", \"dt\"], how=\"left\")\n",
    "    .join(table_zc, on=[\"cust_code\", \"dt\"], how=\"left\")\n",
    ")\n",
    "# TODO: 特征加工\n",
    "\n",
    "data_train_df = data.filter(F.col(\"dt\") <= mid_date)\n",
    "data_test_df = data.filter(F.col(\"dt\") > mid_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table_name = \"algorithm.aip_zq_dixiaohu_custom_feature_train\"\n",
    "test_table_name = \"algorithm.aip_zq_dixiaohu_custom_feature_test\"\n",
    "write_hive(spark, data_train_df, train_table_name, \"dt\")\n",
    "write_hive(spark, data_test_df, test_table_name, \"dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from digitforce.aip.common.utils.hdfs_helper import hdfs_client\n",
    "\n",
    "# 处理分类特征，数值特征，需要丢掉的特征\n",
    "def featuretype_process(data_init, drop_labels, categorical_feature, float_feature):\n",
    "    \"\"\"TODO:自动识别连续特征和离散特征\"\"\"\n",
    "    # Process Feature 1\n",
    "    data_process = data_init.drop(\n",
    "        labels=drop_labels, axis=1, errors=\"ignore\"\n",
    "    )\n",
    "    data_process[categorical_feature] = (\n",
    "        data_process[categorical_feature].astype(\"str\").astype(\"category\")\n",
    "    )\n",
    "    # data_process[float_feature] = data_process[float_feature].astype('float')\n",
    "    data_process[float_feature] = data_process[float_feature].apply(\n",
    "        lambda col: pd.to_numeric(col, errors=\"coerce\"), axis=0\n",
    "    )\n",
    "    return data_process\n",
    "\n",
    "def write_hdfs_path(local_path, hdfs_path):\n",
    "    if hdfs_client.exists(hdfs_path):\n",
    "        hdfs_client.delete(hdfs_path)\n",
    "    hdfs_client.copy_from_local(local_path, hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.05,\n",
    "n_estimators=200,\n",
    "max_depth=5,\n",
    "scale_pos_weight=0.5,\n",
    "is_automl=False,\n",
    "model_and_metrics_data_hdfs_path=None,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    log_loss,\n",
    ")\n",
    "\n",
    "\n",
    "df_train = (\n",
    "    spark\n",
    "    .sql(\n",
    "        f\"\"\"\n",
    "        select * from {train_table_name} limit 1000000\n",
    "        \"\"\"\n",
    "    )\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "print(\"&&&&&&&&&&&&&&&&&&&&&&&\", len(df_train))\n",
    "df_test = (\n",
    "    spark\n",
    "    .sql(\n",
    "        f\"\"\"\n",
    "        select * from {test_table_name}\n",
    "        \"\"\"\n",
    "    )\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "print(\"#######################\", len(df_test))\n",
    "# 连续特征，离散特征，丢弃特征等的处理\n",
    "drop_features = []\n",
    "categorical_features = [\n",
    "    \"gender\",\n",
    "    \"city_name\",\n",
    "    \"province\",\n",
    "    \"educational_degree\",\n",
    "    \"is_login\",\n",
    "]\n",
    "float_features = [\n",
    "    \"age\",\n",
    "    \"jy_num\",\n",
    "    \"jy_rmb\",\n",
    "    \"jygp_num\",\n",
    "    \"jygp_rmb\",\n",
    "    \"jyjj_num\",\n",
    "    \"jyjj_rmb\",\n",
    "    \"ast_total\",\n",
    "    \"ast_fz\",\n",
    "    \"ast_jj\",\n",
    "    \"ast_gp\",\n",
    "    \"ast_zj\",\n",
    "    \"ast_cp\",\n",
    "    \"zc_money\",\n",
    "    \"zr_money\",\n",
    "    \"zc_cnt\",\n",
    "    \"zr_cnt\",\n",
    "]\n",
    "df_train = featuretype_process(\n",
    "    data_init=df_train,\n",
    "    drop_labels=drop_features,\n",
    "    categorical_feature=categorical_features,\n",
    "    float_feature=float_features,\n",
    ")\n",
    "df_test = featuretype_process(\n",
    "    data_init=df_test,\n",
    "    drop_labels=drop_features,\n",
    "    categorical_feature=categorical_features,\n",
    "    float_feature=float_features,\n",
    ")\n",
    "\n",
    "x_train = df_train.drop(columns=[\"cust_code\", \"label\", \"dt\"], axis=1)\n",
    "y_train = df_train[\"label\"]\n",
    "\n",
    "x_test = df_test.drop(columns=[\"cust_code\", \"label\", \"dt\"], axis=1)\n",
    "y_test = df_test[\"label\"]\n",
    "\n",
    "# TODO：mock数据临时修改label\n",
    "random.seed(1234)\n",
    "y_train = y_train.map(lambda x: random.randint(0, 1))\n",
    "y_test = y_test.map(lambda x: random.randint(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 模型训练\n",
    "model = lgb.LGBMClassifier(\n",
    "    boosting_type=\"gbdt\",\n",
    "    objective=\"binary\",\n",
    "    learning_rate=learning_rate,\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric=\"logloss\",\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, verbose=True)\n",
    "\n",
    "\n",
    "# 测试集打分&效果评估\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_score = [x[1] for x in model.predict_proba(x_test)]\n",
    "\n",
    "def getRates(y_test, y_pred, y_pred_score):\n",
    "    s_acc = accuracy_score(y_test, y_pred)\n",
    "    s_auc = roc_auc_score(y_test, y_pred_score)\n",
    "    s_pre = precision_score(y_test, y_pred)\n",
    "    s_rec = recall_score(y_test, y_pred)\n",
    "    s_f1 = f1_score(y_test, y_pred)\n",
    "    s_loss = log_loss(y_test, y_pred_score)\n",
    "    return [s_acc, s_auc, s_pre, s_rec, s_f1, s_loss]\n",
    "\n",
    "all_score = getRates(y_test, y_pred, y_pred_score)\n",
    "print(\"test-logloss={:.4f}, test-auc={:.4f}\".format(all_score[5], all_score[1]))\n",
    "\n",
    "if not is_automl: # automl 默认值这里给False\n",
    "    local_file_path = \"{}_aip_zq_dixiaohu.model\".format(today)\n",
    "    joblib.dump(model, local_file_path)\n",
    "    hdfs_path1 = \"/user/ai/aip/zq/dixiaohu/model/{}.model\".format(today)\n",
    "    hdfs_path2 = \"/user/ai/aip/zq/dixiaohu/model/latest.model\"\n",
    "    write_hdfs_path(local_file_path, hdfs_path1)\n",
    "    write_hdfs_path(local_file_path, hdfs_path2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
