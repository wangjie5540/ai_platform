{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import os\n",
    "import sys\n",
    "file_path=os.getcwd()\n",
    "sys.path.append(file_path)\n",
    "import traceback\n",
    "# from dforce.sf.forecast.sp.spark_bootstrap import predict_new_sku\n",
    "# from dforce.sf.common.local.forcast_replenishment.mysql.common_operation import get_task\n",
    "# from dforce.sf.common.local.common import setup_log\n",
    "# from dforce.sf.forecast.sku_process.sku_category_group import *\n",
    "# # import config as config\n",
    "# from dforce.sf.common.local.common import save_model, load_model\n",
    "# from dforce.sf.main_forecast import spark_forecast_method\n",
    "# sys.path.append(config.sfroot)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyhive import hive\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import isnan, isnull\n",
    "from pyspark.sql import Window\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_rows',1000)\n",
    "\n",
    "# from dforce.sf.main_replenish import spark_replenish_method\n",
    "def spark_init():\n",
    "    \"\"\"\n",
    "    初始化特征\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/data/ibs/anaconda3/bin/python\"\n",
    "    os.environ['PYSPARK_PYTHON']=\"/data/ibs/anaconda3/bin/python\"\n",
    "    spark=SparkSession.builder \\\n",
    "        .appName(\"gxc_test\").master('yarn') \\\n",
    "        .config(\"spark.executor.instances\",\"10\")\\\n",
    "        .config(\"spark.executor.memory\",\"6g\")\\\n",
    "        .config(\"spark.executor.cores\",\"4\")\\\n",
    "        .config(\"spark.driver.memory\", \"4g\")\\\n",
    "        .config(\"spark.driver.maxResultSize\", \"6g\")\\\n",
    "        .config(\"spark.default.parallelism\", \"600\")\\\n",
    "        .config(\"spark.network.timeout\", \"60s\")\\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.adaptive.join.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\", \"128000000\")\\\n",
    "        .config(\"spark.sql.hive.convertMetastoreParquet\", \"false\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "        .config(\"spark.dynamicAllocation.minExecutors\", \"1\")\\\n",
    "        .config(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "    spark.sql(\"set hive.exec.dynamic.partitions=true\")\n",
    "    spark.sql(\"set hive.exec.max.dynamic.partitions=2048\")\n",
    "    spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "    return spark\n",
    "spark = spark_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "sparkdf_order = spark.table(\"ai_dm.ads_scm_goods_sales_log\").\\\n",
    "select(['site_code','goods_code', 'quantity', 'paid_amount','dt']).filter(\"quantity>0\") #~isnull('quantity')\n",
    "sparkdf_order.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dict_rename = {\"site_code\":\"shop_id\",\"goods_code\":\"goods_id\",\"quantity\":\"qty\",\"paid_amount\":\"sale_amt\"}\n",
    "def rename_columns(df, columns):\n",
    "\n",
    "    if isinstance(columns, dict):\n",
    "        for old_name, new_name in columns.items():\n",
    "\n",
    "            df = df.withColumnRenamed(old_name, new_name)\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"'columns' should be a dict, like {'old_name_1':'new_name_1', 'old_name_2':'new_name_2'}\")\n",
    "\n",
    "sparkdf_order = rename_columns(sparkdf_order,dict_rename)\n",
    "sparkdf_order.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark_price = sparkdf_order.groupby(['shop_id','goods_id','dt']).agg({'qty': 'sum','sale_amt':'sum'})\\\n",
    ".withColumnRenamed(\"sum(qty)\",\"sum_qty\")\\\n",
    ".withColumnRenamed(\"sum(sale_amt)\",\"sum_sale_amt\")\n",
    "spark_price = spark_price.withColumn(\"price\",spark_price['sum_sale_amt']/spark_price['sum_qty'])\n",
    "spark_price.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sparkdf_y = spark.table(\"ai_dm_dev.io_daily_y_tmp\").select(['shop_id','goods_id','th_y','dt']).filter(\"y_type='c'\")\n",
    "sparkdf_y.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = spark_price.join(sparkdf_y,['shop_id','goods_id','dt'],'outer').filter(~isnull(\"th_y\"))\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.withColumn(\"ts\", F.unix_timestamp(F.to_timestamp(F.col('dt'), 'yyyyMMdd'),\n",
    "                                                               \"format='yyyy-MM-dd\"))\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.fillna(0, subset=['sum_qty'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def days(i):\n",
    "    return i * 86400\n",
    "w = 10\n",
    "\n",
    "windowOpt = Window.partitionBy(['goods_id','shop_id']).orderBy(F.col('ts')).rangeBetween(start=-days(w - 1),\n",
    "                                                                                end=Window.currentRow)\n",
    "df = df.withColumn(\"{0}_{1}_{2}\".format('price', 'max', w),\n",
    "                             F.max(F.col(\"{0}\".format('price'))).over(windowOpt))\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.withColumn(\"{0}_{1}_{2}\".format('sum_qty', 'sum', w),\n",
    "                             F.sum(F.col(\"{0}\".format('sum_qty'))).over(windowOpt))\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.withColumn(\"{0}_{1}_{2}\".format('sum_sale_amt', 'sum', w),\n",
    "                             F.sum(F.col(\"{0}\".format('sum_sale_amt'))).over(windowOpt))\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.withColumn(\"price_mean\",F.col(\"sum_sale_amt_sum_10\")/F.col(\"sum_qty_sum_10\"))\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.withColumn(\"price\", (F.when((F.col(\"th_y\")>0)&(F.col(\"sum_qty\")==0),F.col('price_mean')).\\\n",
    "                              when((F.col(\"th_y\")==0)&(F.col(\"sum_qty\")==0),F.col('price_max_10')).otherwise(F.col(\"price\"))))\n",
    "df = df.withColumn(\"label_price\", (F.when((F.col(\"th_y\")>0)&(F.col(\"sum_qty\")==0),F.lit('mean')).\\\n",
    "                              when((F.col(\"th_y\")==0)&(F.col(\"sum_qty\")==0),F.lit('max')).otherwise(F.lit(\"ori\"))))\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 最后将真实销量用理论销量来代替\n",
    "df = df.withColumn(\"label_qty\",  F.when((F.col(\"sum_qty\")<F.col(\"th_y\"))|(F.col(\"sum_qty\")==0),F.lit('adjust')).\\\n",
    "              otherwise(F.lit('ori')))\n",
    "df = df.withColumn('sum_qty',\n",
    "              F.when(F.col(\"sum_qty\")<F.col(\"th_y\"),F.col('th_y')).\\\n",
    "              otherwise(F.col('sum_qty')))\n",
    "\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.select(['shop_id','goods_id','dt','sum_qty','price','label_price','label_qty'])\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def is_exist_table(spark,check_table):\n",
    "    \"\"\"\n",
    "    判断表是否存在，存在不为空\n",
    "    :param spark: spark\n",
    "    :param check_table:检测的表\n",
    "    :return: 不存在为空，存在不为空\n",
    "    \"\"\"\n",
    "    result=\"\"\n",
    "    query_sql=\"\"\"\n",
    "        SHOW TABLES LIKE \"{check_table}\"\n",
    "    \"\"\".format(check_table=check_table)\n",
    "    try:\n",
    "        datas=spark.sql(query_sql)\n",
    "        tmp=datas.collect()\n",
    "        if len(tmp)>0:\n",
    "            result=tmp[0][1]\n",
    "    except:\n",
    "        pass\n",
    "    return result\n",
    "\n",
    "def write_to_hive(spark,data,partition,table_name,mode_type):\n",
    "    \"\"\"\n",
    "    写入hive\n",
    "    :param spark: spark\n",
    "    :param data: 数据\n",
    "    :param partition: partition\n",
    "    :param table_name: 数据名\n",
    "    :param mode_type: 方式：overwrite、append\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if is_exist_table(spark,table_name) != \"\":  # 表示表存在\n",
    "        data.write.mode(mode_type).insertInto(table_name, True)  # 表示覆盖分区\n",
    "    else:\n",
    "        if partition!=[]:\n",
    "            data.write.partitionBy(partition).mode(mode_type).saveAsTable(table_name)\n",
    "        else:\n",
    "            data.write.mode(mode_type).saveAsTable(table_name)\n",
    "\n",
    "write_to_hive(spark,df,'dt','ai_dm_dev.price_elastic_mid_d','overwrite')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}