{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from digitforce.aip.common.utils.spark_helper import SparkClient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "sp = SparkClient().get_session()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      databaseName|\n",
      "+------------------+\n",
      "|             ai_dm|\n",
      "|         ai_dm_dev|\n",
      "|               aip|\n",
      "|         algorithm|\n",
      "|        cj_pa_test|\n",
      "|         d_cutomer|\n",
      "|           default|\n",
      "|    dio_flink_test|\n",
      "|             dwd_l|\n",
      "|     forecast_algo|\n",
      "| forecast_algo_dev|\n",
      "|         hive_test|\n",
      "|           hm_test|\n",
      "|               ibs|\n",
      "|        kylin_test|\n",
      "|            labelx|\n",
      "|               lbs|\n",
      "|             ln_xp|\n",
      "|matedata_test_hive|\n",
      "|    metadatax_test|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp.sql('show databases').show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#!/user/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "from typing import Dict\n",
    "# import digitforce.aip.common.utils.spark_helper as SparkEnv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from typing import List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import digitforce.aip.common.utils.spark_helper as Spark\n",
    "\n",
    "def spark_read(spark, table_name, tempViewName):\n",
    "    df = spark.read.format('starrocks').option('starrocks.table.identifier', f'{table_name}').option(\n",
    "        'starrocks.fenodes', '172.21.32.16:8030').option('user', 'root').option('password', '').load()\n",
    "    df.createOrReplaceTempView(f'{tempViewName}')\n",
    "    print(f'success creating temp view: {tempViewName}')\n",
    "\n",
    "class CreateDataset:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def ConstructFeatures(self,\n",
    "                          train_period: str,\n",
    "                          predict_period: str,\n",
    "                          cat_list: str,\n",
    "                          is_train,\n",
    "                          order_table: str,\n",
    "                          bh_table: str,\n",
    "                          user_table: str,\n",
    "                          item_table: str,\n",
    "                          odinfo: Dict,\n",
    "                          bhinfo: Dict,\n",
    "                          usinfo: Dict,\n",
    "                          itinfo: Dict,\n",
    "                          bh_code_map: Dict,\n",
    "                          where_sql: str\n",
    "                          ):\n",
    "        # today = datetime.datetime.today()\n",
    "        spark = Spark.SparkClient().get_session()\n",
    "        # spark = None\n",
    "\n",
    "        if is_train:\n",
    "            cur_str = get_min_date_of_4_table(spark, user_table, order_table, item_table, bh_table, usinfo, odinfo,\n",
    "                                              itinfo, bhinfo)\n",
    "            # print(cur_str)\n",
    "            cur_str = (datetime.datetime.strptime(cur_str, '%Y-%m-%d') - datetime.timedelta(days=30)).strftime(\n",
    "                '%Y-%m-%d')\n",
    "            # print(cur_str)\n",
    "            feature_dates, train_date, predict_date = get_time_params(cur_str, train_period, predict_period)\n",
    "            order_table, user_table, item_table, bh_table = 'order_table_tmp', 'user_table_tmp', 'item_table_tmp', 'bh_table_tmp'\n",
    "            samples = get_samples_train(spark, order_table, odinfo, item_table, itinfo, cur_str, train_date, cat_list,\n",
    "                                        predict_date)\n",
    "            # print(samples)\n",
    "        else:\n",
    "            # cur_str = today.strftime('%Y-%m-%d')\n",
    "            cur_str = get_min_date_of_4_table(spark, user_table, order_table, item_table, bh_table, usinfo, odinfo,\n",
    "                                              itinfo, bhinfo)\n",
    "            feature_dates, train_date, predict_date = get_time_params(cur_str, train_period, predict_period)\n",
    "            #             print(feature_dates, train_date, predict_date )\n",
    "            samples = get_samples_predict(spark, order_table, odinfo, item_table, itinfo, cat_list, train_date, cur_str,\n",
    "                                          where_sql, user_table, usinfo)\n",
    "        #             print(samples)\n",
    "        if len(samples) == 0:\n",
    "            spark.stop()\n",
    "            return samples\n",
    "        else:\n",
    "            features_of_order = get_order_features(spark, feature_dates, cur_str, order_table, odinfo, item_table,\n",
    "                                                   itinfo, cat_list)\n",
    "            # print(features_of_order)\n",
    "            features_of_user = get_label_features(spark, samples, user_table, usinfo, cur_str)\n",
    "            # print(features_of_user)\n",
    "            features_of_behavior = get_behavior_features(spark, feature_dates, item_table, itinfo, bh_table, bhinfo,\n",
    "                                                         bh_code_map, cur_str, cat_list)\n",
    "            # print(features_of_behavior)\n",
    "            if not features_of_order.empty:\n",
    "                data = pd.merge(samples, features_of_order, how='left', on='user_id')\n",
    "            else:\n",
    "                data = samples\n",
    "            if not features_of_user.empty:\n",
    "                data = pd.merge(data, features_of_user, how='left', on='user_id')\n",
    "            if not features_of_behavior.empty:\n",
    "                data = pd.merge(data, features_of_behavior, how='left', on='user_id')\n",
    "\n",
    "            spark.stop()\n",
    "            return data\n",
    "\n",
    "\n",
    "def get_min_date_of_4_table(spark, user_table: str, order_table: str, item_table: str, bh_table: str, usinfo: Dict,\n",
    "                            odinfo: Dict, itinfo: Dict, bhinfo: Dict):\n",
    "    sql1 = '''select max({0}) as us_max_date from {1} '''.format(usinfo['dt'], user_table)\n",
    "    sql2 = '''select substr(max({0}),1,10) as od_max_date from {1} '''.format(odinfo['order_time'], order_table)\n",
    "    sql3 = '''select max({0}) as it_max_date from {1} '''.format(itinfo['dt'], item_table)\n",
    "    sql4 = '''select substr(max({0}),1,10) as bh_max_date from {1} '''.format(bhinfo['event_time'], bh_table)\n",
    "    spark_read(spark, user_table, 'user_table_tmp')\n",
    "    spark_read(spark, order_table, 'order_table_tmp')\n",
    "    spark_read(spark, item_table, 'item_table_tmp')\n",
    "    spark_read(spark, bh_table, 'bh_table_tmp')\n",
    "    dt1 = spark.sql(sql1).toPandas()\n",
    "    dt2 = spark.sql(sql2).toPandas()\n",
    "    dt3 = spark.sql(sql3).toPandas()\n",
    "    dt4 = spark.sql(sql4).toPandas()\n",
    "    min_date = min([dt1.iloc[0, 0], dt2.iloc[0, 0], dt3.iloc[0, 0], dt4.iloc[0, 0]])\n",
    "    return min_date\n",
    "\n",
    "\n",
    "def get_time_params(cur_str: str, train_period: str, predict_period: str):\n",
    "    current_day = datetime.datetime.strptime(cur_str, \"%Y-%m-%d\")\n",
    "    last_3_days = (current_day - datetime.timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "    last_7_days = (current_day - datetime.timedelta(days=6)).strftime(\"%Y-%m-%d\")\n",
    "    last_15_days = (current_day - datetime.timedelta(days=14)).strftime(\"%Y-%m-%d\")\n",
    "    last_1_month = (current_day - datetime.timedelta(days=29)).strftime(\"%Y-%m-%d\")\n",
    "    last_2_month = (current_day - datetime.timedelta(days=59)).strftime(\"%Y-%m-%d\")\n",
    "    next_day = (current_day + datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    next_3_day = (current_day + datetime.timedelta(days=3)).strftime(\"%Y-%m-%d\")\n",
    "    next_7_day = (current_day + datetime.timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "    next_15_day = (current_day + datetime.timedelta(days=15)).strftime(\"%Y-%m-%d\")\n",
    "    next_1_month = (current_day + datetime.timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
    "    next_2_month = (current_day + datetime.timedelta(days=60)).strftime(\"%Y-%m-%d\")\n",
    "    if train_period == '过去15天':\n",
    "        train_date = last_15_days\n",
    "    elif train_period == '过去30天':\n",
    "        train_date = last_1_month\n",
    "    else:\n",
    "        train_date = last_2_month\n",
    "    if predict_period == '未来15天':\n",
    "        predict_date = next_15_day\n",
    "    elif predict_period == '未来30天':\n",
    "        predict_date = next_1_month\n",
    "    else:\n",
    "        predict_date = next_2_month\n",
    "    feature_dates = [last_2_month, last_1_month, last_15_days, last_7_days, last_3_days]\n",
    "    return feature_dates, train_date, predict_date\n",
    "\n",
    "\n",
    "def get_samples_train(spark, order_table, odinfo_map, item_table, itinfo_map, cur_str, train_date, cate_list,\n",
    "                      predict_date):\n",
    "    sql = '''\n",
    "        select\n",
    "            a.user_id,\n",
    "            if(b.user_id is null, 0, 1) as label\n",
    "        from\n",
    "        (\n",
    "            select c.user_id as user_id from\n",
    "            (select\n",
    "                {0} as user_id,\n",
    "                {1} as sku\n",
    "            from\n",
    "                {2}\n",
    "            where\n",
    "                {3} between '{4}' and '{5}'\n",
    "            group by {0},{1})as c\n",
    "            left join\n",
    "            (select\n",
    "                {6} as sku,\n",
    "                {7} as cat\n",
    "            from\n",
    "                {8}\n",
    "            where {9} = '{5}'\n",
    "                and {7} in {10})as d\n",
    "            on c.sku=d.sku\n",
    "            where d.sku is not null\n",
    "            group by c.user_id\n",
    "        ) as a\n",
    "        left join\n",
    "        (\n",
    "            select e.user_id as user_id from\n",
    "            (select\n",
    "                {0} as user_id,\n",
    "                {1} as sku\n",
    "            from\n",
    "                {2}\n",
    "            where\n",
    "                {3} > '{5}' and {3} <= '{11}'\n",
    "            group by {0}, {1})as e\n",
    "            left join\n",
    "            (select\n",
    "                {6} as sku,\n",
    "                {7} as cat\n",
    "            from\n",
    "                {8}\n",
    "            where {9} = '{11}'\n",
    "                and {7} in {10})as f\n",
    "            on e.sku=f.sku\n",
    "            where f.sku is not null\n",
    "            group by e.user_id\n",
    "        )as b\n",
    "        on a.user_id = b.user_id\n",
    "    '''.format(odinfo_map['user_id'], odinfo_map['sku'], order_table, odinfo_map['order_time'], train_date, cur_str,\n",
    "               itinfo_map['sku'], itinfo_map['cate'], item_table, itinfo_map['dt'], cate_list, predict_date)\n",
    "    #     print(sql)\n",
    "    samples = spark.sql(sql).toPandas()\n",
    "    return samples\n",
    "\n",
    "\n",
    "def get_samples_predict(spark, order_table, odinfo_map, item_table, itinfo_map, cate_list, train_date, cur_str,\n",
    "                        where_sql, user_table, usinfo):\n",
    "    sql = '''\n",
    "        select\n",
    "            a.user_id\n",
    "        from\n",
    "        (select\n",
    "            {0} as user_id,\n",
    "            {1} as sku\n",
    "        from\n",
    "            {2}\n",
    "        where\n",
    "            {3} between '{4}' and '{5}'\n",
    "        group by {0},{1}) as a\n",
    "        left join\n",
    "        (select\n",
    "            {6} as sku\n",
    "        from\n",
    "            {7}\n",
    "        where\n",
    "            {8} = '{5}'\n",
    "            and {9} in {10}) as b\n",
    "        on a.sku=b.sku\n",
    "        where b.sku is not null\n",
    "        group by a.user_id\n",
    "    '''.format(odinfo_map['user_id'], odinfo_map['sku'], order_table, odinfo_map['order_time'], train_date, cur_str,\n",
    "               itinfo_map['sku'], item_table, itinfo_map['dt'], itinfo_map['cate'], cate_list)\n",
    "    #     print(sql)\n",
    "    samples = spark.sql(sql).toPandas()\n",
    "    if where_sql:\n",
    "        sql_where = '''\n",
    "            select {0} as user_id\n",
    "            from {1}\n",
    "            where {2}\n",
    "                and {3} = '{4}'\n",
    "        '''.format(usinfo['user_id'], user_table, where_sql, usinfo['dt'], cur_str)\n",
    "        samples1 = spark.sql(sql_where).toPandas()\n",
    "        if not samples.empty and not samples1.empty:\n",
    "            samples = pd.merge(samples1, samples, how='left', on='user_id')\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def get_order_features(spark, feature_dates: List[str], cur_str, order_table, odinfo_map, item_table, itinfo_map,\n",
    "                       cate_list):\n",
    "    last_x_days_name = ['2m', '1m', '15d', '7d', '3d']\n",
    "    last_x_days = [60, 30, 15, 7, 3]\n",
    "    orders_df = pd.DataFrame()\n",
    "    sql_item = '''\n",
    "        select\n",
    "            {0} as sku\n",
    "        from\n",
    "            {1}\n",
    "        where\n",
    "            {2} = '{3}'\n",
    "            and {4} in {5}\n",
    "    '''.format(itinfo_map['sku'], item_table, itinfo_map['dt'], cur_str, itinfo_map['cate'], cate_list)\n",
    "    #     print(sql_item)\n",
    "    item_tmp = spark.sql(sql_item)\n",
    "    item_tmp.createOrReplaceTempView('item_cat_tmp')\n",
    "\n",
    "    for i in range(len(feature_dates)):\n",
    "        last_x_day = feature_dates[i]\n",
    "        #         print(last_x_days[i])\n",
    "        # 计算近x天的订单量，购买商品种类数目，平均购买间隔（=（最大购买时间-最小购买时间）/购买天数）\n",
    "        sql_orders_count = '''\n",
    "            select\n",
    "                {0} as user_id,\n",
    "                count(distinct {1}) as {2},\n",
    "                if(count(distinct {1})=1, {3}, round(datediff(from_unixtime(unix_timestamp(substr(max({4}),1,10),\"yyyy-MM-dd\"),\"yyyy-MM-dd\"),from_unixtime(unix_timestamp(substr(min({4}),1,10),\"yyyy-MM-dd\"),\"yyyy-MM-dd\"))/(count(distinct substr({4},1,10))-1),2)) as {5} --时间分区可能会变\n",
    "            from {6}\n",
    "            where {4} between '{7}' and '{8}'\n",
    "                and {9} in (select sku from item_cat_tmp)\n",
    "            group by {0}\n",
    "        '''.format(odinfo_map['user_id'], odinfo_map['order_id'], 'od_cat_ct_' + last_x_days_name[i],\n",
    "                   last_x_days[i] / 2, odinfo_map['order_time'], 'avg_cat_jg_' + last_x_days_name[i], order_table,\n",
    "                   last_x_day, cur_str, odinfo_map['sku'])\n",
    "        #         print(sql_orders_count)\n",
    "        orders_count_df = spark.sql(sql_orders_count).toPandas()\n",
    "        orders_count_df[['user_id']] = orders_count_df[['user_id']].astype(str)\n",
    "        orders_count_df[['od_cat_ct_' + last_x_days_name[i], 'avg_cat_jg_' + last_x_days_name[i]]] = orders_count_df[\n",
    "            ['od_cat_ct_' + last_x_days_name[i], 'avg_cat_jg_' + last_x_days_name[i]]].astype(np.float)\n",
    "\n",
    "        if orders_df.empty:\n",
    "            orders_df = orders_count_df\n",
    "        else:\n",
    "            orders_df = pd.merge(orders_df, orders_count_df, how='outer', on='user_id')\n",
    "        #         print(orders_df)\n",
    "\n",
    "        if odinfo_map['sale_quantity']:\n",
    "            # 计算近x天的购买商品数量，平均购买商品数量\n",
    "            sql_order_qty = '''\n",
    "                select\n",
    "                    {0} as user_id,\n",
    "                    sum(coalesce({1}, 0)) as {2},\n",
    "                    sum(coalesce({1},0)) / count(distinct {3}) as {4}\n",
    "                from {5}\n",
    "                where {6} between '{7}' and '{8}'\n",
    "                    and {9} in (select sku from item_cat_tmp)\n",
    "                group by {0}\n",
    "            '''.format(odinfo_map['user_id'], odinfo_map['sale_quantity'], 'qty_cat_' + last_x_days_name[i],\n",
    "                       odinfo_map['order_id'], 'qty_cat_avg_' + last_x_days_name[i], order_table,\n",
    "                       odinfo_map['order_time'], last_x_day, cur_str, odinfo_map['sku'])\n",
    "            #             print(sql_order_qty)\n",
    "            order_qty_df = spark.sql(sql_order_qty).toPandas()\n",
    "            order_qty_df[['user_id']] = order_qty_df[['user_id']].astype(str)\n",
    "            order_qty_df[['qty_cat_' + last_x_days_name[i], 'qty_cat_avg_' + last_x_days_name[i]]] = order_qty_df[\n",
    "                ['qty_cat_' + last_x_days_name[i], 'qty_cat_avg_' + last_x_days_name[i]]].astype(np.float)\n",
    "            orders_df = pd.merge(orders_df, order_qty_df, how='outer', on='user_id')\n",
    "\n",
    "        if odinfo_map['sale_amount']:\n",
    "            # 计算近x天的购买金额，平均购买金额\n",
    "            sql_order_amt = '''\n",
    "                select\n",
    "                    {0} as user_id,\n",
    "                    sum(coalesce({1}, 0)) as {2},\n",
    "                    sum(coalesce({1},0)) / count(distinct {3}) as {4}\n",
    "                from {5}\n",
    "                where {6} between '{7}' and '{8}'\n",
    "                    and {9} in (select sku from item_cat_tmp)\n",
    "                group by {0}\n",
    "            '''.format(odinfo_map['user_id'], odinfo_map['sale_amount'], 'amt_cat_' + last_x_days_name[i],\n",
    "                       odinfo_map['order_id'], 'amt_cat_avg_' + last_x_days_name[i], order_table,\n",
    "                       odinfo_map['order_time'], last_x_day, cur_str, odinfo_map['sku'])\n",
    "            #             print(sql_order_amt)\n",
    "            order_amt_df = spark.sql(sql_order_amt).toPandas()\n",
    "            order_amt_df[['user_id']] = order_amt_df[['user_id']].astype(str)\n",
    "            order_amt_df[['amt_cat_' + last_x_days_name[i], 'amt_cat_avg_' + last_x_days_name[i]]] = order_amt_df[\n",
    "                ['amt_cat_' + last_x_days_name[i], 'amt_cat_avg_' + last_x_days_name[i]]].astype(np.float)\n",
    "            orders_df = pd.merge(orders_df, order_amt_df, how='outer', on='user_id')\n",
    "\n",
    "        if i == 0:\n",
    "            # 对近60天求最近末次购买距今天数\n",
    "            sql_order_lastbuy = '''\n",
    "                select\n",
    "                    {0} as user_id,\n",
    "                    if(count(distinct {1})=0, 60, round(datediff(from_unixtime(unix_timestamp(\"{2}\",\"yyyy-MM-dd\"),\"yyyy-MM-dd\"),from_unixtime(unix_timestamp(substr(max({1}),1,10),\"yyyy-MM-dd\"),\"yyyy-MM-dd\")),2)) as lastbuy_diff\n",
    "                from {3}\n",
    "                where {1} between '{4}' and '{2}'\n",
    "                    and {5} in (select sku from item_cat_tmp)\n",
    "                group by {0}\n",
    "            '''.format(odinfo_map['user_id'], odinfo_map['order_time'], cur_str, order_table, last_x_day,\n",
    "                       odinfo_map['sku'])\n",
    "            #             print(sql_order_lastbuy)\n",
    "            order_lastbuy_df = spark.sql(sql_order_lastbuy).toPandas()\n",
    "            order_lastbuy_df[['user_id']] = order_lastbuy_df[['user_id']].astype(str)\n",
    "            order_lastbuy_df[['lastbuy_diff']] = order_lastbuy_df[['lastbuy_diff']].astype(np.float)\n",
    "            orders_df = pd.merge(orders_df, order_lastbuy_df, how='outer', on='user_id')\n",
    "\n",
    "        # 以下特征为全品类：即不限制品类\n",
    "        # 计算近x天的订单量，购买商品种类数目，平均购买间隔（=（最大购买时间-最小购买时间）/购买天数）\n",
    "        sql_orders_count_all = '''\n",
    "            select\n",
    "                {0} as user_id,\n",
    "                count(distinct {1}) as {2},\n",
    "                if(count(distinct {1})=1, {3}, round(datediff(from_unixtime(unix_timestamp(substr(max({4}),1,10),\"yyyy-MM-dd\"),\"yyyy-MM-dd\"),from_unixtime(unix_timestamp(substr(min({4}),1,10),\"yyyy-MM-dd\"),\"yyyy-MM-dd\"))/(count(distinct substr({4},1,10))-1),2)) as {5} --时间分区可能会变\n",
    "            from {6}\n",
    "            where {4} between '{7}' and '{8}'\n",
    "            group by {0}\n",
    "        '''.format(odinfo_map['user_id'], odinfo_map['order_id'], 'od_ct_' + last_x_days_name[i], last_x_days[i] / 2,\n",
    "                   odinfo_map['order_time'], 'avg_jg_' + last_x_days_name[i], order_table, last_x_day, cur_str)\n",
    "        orders_count_all_df = spark.sql(sql_orders_count_all).toPandas()\n",
    "        orders_count_all_df[['user_id']] = orders_count_all_df[['user_id']].astype(str)\n",
    "        orders_count_all_df[['od_ct_' + last_x_days_name[i], 'avg_jg_' + last_x_days_name[i]]] = orders_count_all_df[\n",
    "            ['od_ct_' + last_x_days_name[i], 'avg_jg_' + last_x_days_name[i]]].astype(np.float)\n",
    "        if orders_df.empty:\n",
    "            orders_df = orders_count_all_df\n",
    "        else:\n",
    "            orders_df = pd.merge(orders_df, orders_count_all_df, how='outer', on='user_id')\n",
    "\n",
    "        if odinfo_map['sale_quantity']:\n",
    "            # 计算近x天的购买商品数量，平均购买商品数量\n",
    "            sql_order_qty_all = '''\n",
    "                select\n",
    "                    {0} as user_id,\n",
    "                    sum(coalesce({1}, 0)) as {2},\n",
    "                    sum(coalesce({1},0)) / count(distinct {3}) as {4}\n",
    "                from {5}\n",
    "                where {6} between '{7}' and '{8}'\n",
    "                group by {0}\n",
    "            '''.format(odinfo_map['user_id'], odinfo_map['sale_quantity'], 'qty_' + last_x_days_name[i],\n",
    "                       odinfo_map['order_id'], 'qty_avg_' + last_x_days_name[i], order_table, odinfo_map['order_time'],\n",
    "                       last_x_day, cur_str)\n",
    "            order_qty_all_df = spark.sql(sql_order_qty_all).toPandas()\n",
    "            order_qty_all_df[['user_id']] = order_qty_all_df[['user_id']].astype(str)\n",
    "            order_qty_all_df[['qty_' + last_x_days_name[i], 'qty_avg_' + last_x_days_name[i]]] = order_qty_all_df[\n",
    "                ['qty_' + last_x_days_name[i], 'qty_avg_' + last_x_days_name[i]]].astype(np.float)\n",
    "            orders_df = pd.merge(orders_df, order_qty_all_df, how='outer', on='user_id')\n",
    "\n",
    "        if odinfo_map['sale_amount']:\n",
    "            # 计算近x天的购买金额，平均购买金额\n",
    "            sql_order_amt_all = '''\n",
    "                select\n",
    "                    {0} as user_id,\n",
    "                    sum(coalesce({1}, 0)) as {2},\n",
    "                    sum(coalesce({1},0)) / count(distinct {3}) as {4}\n",
    "                from {5}\n",
    "                where {6} between '{7}' and '{8}'\n",
    "                group by {0}\n",
    "            '''.format(odinfo_map['user_id'], odinfo_map['sale_amount'], 'amt_' + last_x_days_name[i],\n",
    "                       odinfo_map['order_id'], 'amt_avg_' + last_x_days_name[i], order_table, odinfo_map['order_time'],\n",
    "                       last_x_day, cur_str)\n",
    "            order_amt_all_df = spark.sql(sql_order_amt_all).toPandas()\n",
    "            order_amt_all_df[['user_id']] = order_amt_all_df[['user_id']].astype(str)\n",
    "            order_amt_all_df[['amt_' + last_x_days_name[i], 'amt_avg_' + last_x_days_name[i]]] = order_amt_all_df[\n",
    "                ['amt_' + last_x_days_name[i], 'amt_avg_' + last_x_days_name[i]]].astype(np.float)\n",
    "            orders_df = pd.merge(orders_df, order_amt_all_df, how='outer', on='user_id')\n",
    "    #     print(orders_df)\n",
    "    return orders_df\n",
    "\n",
    "\n",
    "def get_behavior_features(spark, feature_dates: List[str], item_table, itinfo_map, behavior_table, bhinfo_map,\n",
    "                          bh_code_map, cur_str, cate_list):\n",
    "    last_x_days_name = ['2m', '1m', '15d', '7d', '3d']\n",
    "    bh_df = pd.DataFrame()\n",
    "    sql_item = '''\n",
    "        select\n",
    "            {0} as sku\n",
    "        from\n",
    "            {1}\n",
    "        where\n",
    "            {2} = '{3}'\n",
    "            and {4} in {5}\n",
    "    '''.format(itinfo_map['sku'], item_table, itinfo_map['dt'], cur_str, itinfo_map['cate'], cate_list)\n",
    "    #     print(sql_item)\n",
    "    item_tmp = spark.sql(sql_item)\n",
    "    item_tmp.createOrReplaceTempView('item_cat_tmp')\n",
    "\n",
    "    for i in range(len(feature_dates)):\n",
    "        last_x_day = feature_dates[i]\n",
    "        if bh_code_map['click']:\n",
    "            sql_dianji_cat = '''\n",
    "                select\n",
    "                    {0} as user_id,\n",
    "                    count({1}) as {2}\n",
    "                from\n",
    "                    {3}\n",
    "                where\n",
    "                    {4} between '{5}' and '{6}' -- Time\n",
    "                    and {1}='{7}' -- event_code\n",
    "                    and {8} in (select sku from item_cat_tmp) -- sku\n",
    "                group by {0}\n",
    "            '''.format(bhinfo_map['user_id'], bhinfo_map['event_code'], 'dianji_cat_' + last_x_days_name[i],\n",
    "                       behavior_table, bhinfo_map['event_time'], last_x_day, cur_str, bh_code_map['click'],\n",
    "                       bhinfo_map['sku'])\n",
    "\n",
    "            sql_dianji_all = '''\n",
    "                select\n",
    "                    {0} as user_id,\n",
    "                    count({1}) as {2}\n",
    "                from\n",
    "                    {3}\n",
    "                where\n",
    "                    {4} between '{5}' and '{6}' -- Time\n",
    "                    and {1}='{7}' -- event_code\n",
    "                group by {0}\n",
    "            '''.format(bhinfo_map['user_id'], bhinfo_map['event_code'], 'dianji_all_' + last_x_days_name[i],\n",
    "                       behavior_table, bhinfo_map['event_time'], last_x_day, cur_str, bh_code_map['click'])\n",
    "            #             print(sql_dianji_cat)\n",
    "            #             print(sql_dianji_all)\n",
    "            dianji_cat_df = spark.sql(sql_dianji_cat).toPandas()\n",
    "            dianji_cat_df[['user_id']] = dianji_cat_df[['user_id']].astype(str)\n",
    "            dianji_cat_df[['dianji_cat_' + last_x_days_name[i]]] = dianji_cat_df[['dianji_cat_' + last_x_days_name[i]]].astype(\n",
    "                np.int)\n",
    "            dianji_all_df = spark.sql(sql_dianji_all).toPandas()\n",
    "            dianji_all_df[['user_id']] = dianji_all_df[['user_id']].astype(str)\n",
    "            dianji_all_df['dianji_all_' + last_x_days_name[i]] = dianji_all_df[['dianji_all_' + last_x_days_name[i]]].astype(\n",
    "                np.int)\n",
    "\n",
    "            if bh_df.empty:\n",
    "                bh_df = dianji_cat_df\n",
    "            else:\n",
    "                bh_df = pd.merge(bh_df, dianji_cat_df, how='outer', on='user_id')\n",
    "                bh_df = pd.merge(bh_df, dianji_all_df, how='outer', on='user_id')\n",
    "\n",
    "        if bh_code_map['cart_add']:\n",
    "            sql_jiagou_cat = '''\n",
    "                select\n",
    "                    {0} as user_id,\n",
    "                    count({1}) as {2}\n",
    "                from\n",
    "                    {3}\n",
    "                where\n",
    "                    {4} between '{5}' and '{6}' -- Time\n",
    "                    and {1}='{7}' -- event_code\n",
    "                    and {8} in (select sku from item_cat_tmp) -- sku\n",
    "                group by {0}\n",
    "            '''.format(bhinfo_map['user_id'], bhinfo_map['event_code'], 'jiagou_cat_' + last_x_days_name[i],\n",
    "                       behavior_table, bhinfo_map['event_time'], last_x_day, cur_str, bh_code_map['cart_add'],\n",
    "                       bhinfo_map['sku'])\n",
    "            sql_jiagou_all = '''\n",
    "                select\n",
    "                    {0} as user_id,\n",
    "                    count({1}) as {2}\n",
    "                from\n",
    "                    {3}\n",
    "                where\n",
    "                    {4} between '{5}' and '{6}' -- Time\n",
    "                    and {1}='{7}' -- event_code\n",
    "               group by {0}\n",
    "            '''.format(bhinfo_map['user_id'], bhinfo_map['event_code'], 'jiagou_all_' + last_x_days_name[i],\n",
    "                       behavior_table, bhinfo_map['event_time'], last_x_day, cur_str, bh_code_map['cart_add'])\n",
    "            #             print(sql_jiagou_cat)\n",
    "            #             print(sql_jiagou_all)\n",
    "            jiagou_cat_df = spark.sql(sql_jiagou_cat).toPandas()\n",
    "            jiagou_cat_df[['user_id']] = jiagou_cat_df[['user_id']].astype(str)\n",
    "            jiagou_cat_df[['jiagou_cat_' + last_x_days_name[i]]] = jiagou_cat_df[['jiagou_cat_' + last_x_days_name[i]]].astype(\n",
    "                np.int)\n",
    "            jiagou_all_df = spark.sql(sql_jiagou_all).toPandas()\n",
    "            jiagou_all_df[['user_id']] = jiagou_all_df[['user_id']].astype(str)\n",
    "            jiagou_all_df['jiagou_all_' + last_x_days_name[i]] = jiagou_all_df[['jiagou_all_' + last_x_days_name[i]]].astype(\n",
    "                np.int)\n",
    "\n",
    "            if bh_df.empty:\n",
    "                bh_df = jiagou_cat_df\n",
    "            else:\n",
    "                bh_df = pd.merge(bh_df, jiagou_cat_df, how='outer', on='user_id')\n",
    "                bh_df = pd.merge(bh_df, jiagou_all_df, how='outer', on='user_id')\n",
    "\n",
    "        if bh_code_map['browse']:\n",
    "            sql_view_cat = '''\n",
    "                select\n",
    "                    {0} as user_id,\n",
    "                    count({1}) as {2}\n",
    "                from\n",
    "                    {3}\n",
    "                where\n",
    "                    {4} between '{5}' and '{6}' -- Time\n",
    "                    and {1}='{7}' -- event_code\n",
    "                    and {8} in (select sku from item_cat_tmp) -- sku\n",
    "                group by {0}\n",
    "            '''.format(bhinfo_map['user_id'], bhinfo_map['event_code'], 'view_cat_' + last_x_days_name[i],\n",
    "                       behavior_table, bhinfo_map['event_time'], last_x_day, cur_str, bh_code_map['browse'],\n",
    "                       bhinfo_map['sku'])\n",
    "            sql_view_all = '''\n",
    "                select\n",
    "                    {0} as user_id,\n",
    "                    count({1}) as {2}\n",
    "                from\n",
    "                    {3}\n",
    "                where\n",
    "                    {4} between '{5}' and '{6}' -- Time\n",
    "                    and {1}='{7}' -- event_code\n",
    "                group by {0}\n",
    "            '''.format(bhinfo_map['user_id'], bhinfo_map['event_code'], 'view_all_' + last_x_days_name[i],\n",
    "                       behavior_table, bhinfo_map['event_time'], last_x_day, cur_str, bh_code_map['browse'])\n",
    "            view_cat_df = spark.sql(sql_view_cat).toPandas()\n",
    "            view_cat_df[['user_id']] = view_cat_df[['user_id']].astype(str)\n",
    "            view_cat_df[['view_cat_' + last_x_days_name[i]]] = view_cat_df[['view_cat_' + last_x_days_name[i]]].astype(np.int)\n",
    "            view_all_df = spark.sql(sql_view_all).toPandas()\n",
    "            view_all_df[['user_id']] = view_all_df[['user_id']].astype(str)\n",
    "            view_all_df['view_all_' + last_x_days_name[i]] = view_all_df[['view_all_' + last_x_days_name[i]]].astype(np.int)\n",
    "\n",
    "            if bh_df.empty:\n",
    "                bh_df = view_cat_df\n",
    "            else:\n",
    "                bh_df = pd.merge(bh_df, view_cat_df, how='outer', on='user_id')\n",
    "                bh_df = pd.merge(bh_df, view_all_df, how='outer', on='user_id')\n",
    "    #     print(bh_df)\n",
    "    return bh_df\n",
    "\n",
    "\n",
    "def get_label_features(spark, samples: pd.DataFrame, user_table, usinfo_map, cur_str):\n",
    "    sample_spark_df = spark.createDataFrame(samples)\n",
    "    sample_spark_df.createOrReplaceTempView('samples_tmp')\n",
    "    # print(spark.sql('select * from samples_tmp').head(50))\n",
    "    userlabel_df = pd.DataFrame()\n",
    "    le = LabelEncoder()\n",
    "    # 待改进：可合并\n",
    "    if ('sex' in usinfo_map) and usinfo_map['sex'] != '':\n",
    "        sql_gender = '''\n",
    "             select\n",
    "                a.user_id,\n",
    "                b.gender\n",
    "            from\n",
    "                (select distinct user_id from samples_tmp) as a\n",
    "            left join\n",
    "                (select\n",
    "                    {0} as user_id,\n",
    "                    {1} as gender\n",
    "                from {2}\n",
    "                where {3} = '{4}' ) as b\n",
    "            on a.user_id = b.user_id\n",
    "\n",
    "        '''.format(usinfo_map['user_id'], usinfo_map['sex'], user_table, usinfo_map['dt'], cur_str)\n",
    "        # print(sql_gender)\n",
    "        gender_df = spark.sql(sql_gender).toPandas()\n",
    "        gender_df.fillna(value=0, inplace=True)\n",
    "        gender_df[['user_id', 'gender']] = gender_df[['user_id', 'gender']].astype(str)\n",
    "        if userlabel_df.empty:\n",
    "            userlabel_df = gender_df\n",
    "        else:\n",
    "            userlabel_df = pd.merge(userlabel_df, gender_df, how='inner', on='user_id')\n",
    "        userlabel_df['gender'] = le.fit_transform(userlabel_df['gender'])\n",
    "\n",
    "    if ('age' in usinfo_map) and usinfo_map['age'] != '':\n",
    "        sql_age = '''\n",
    "            select\n",
    "                a.user_id,\n",
    "                b.age\n",
    "            from\n",
    "                (select user_id from samples_tmp) as a\n",
    "            left join\n",
    "                (select\n",
    "                    {0} as user_id,\n",
    "                    {1} as age\n",
    "                from {2}\n",
    "                where {3} = '{4}' ) as b\n",
    "            on a.user_id = b.user_id\n",
    "\n",
    "        '''.format(usinfo_map['user_id'], usinfo_map['age'], user_table, usinfo_map['dt'], cur_str)\n",
    "        age_df = spark.sql(sql_age).toPandas()\n",
    "        age_df.fillna(value=0, inplace=True)\n",
    "        age_df[['user_id']] = age_df[['user_id']].astype(str)\n",
    "        age_df[['age']] = age_df[['age']].astype(np.int64)\n",
    "        if userlabel_df.empty:\n",
    "            userlabel_df = age_df\n",
    "        else:\n",
    "            userlabel_df = pd.merge(userlabel_df, age_df, how='inner', on='user_id')\n",
    "\n",
    "    if ('city' in usinfo_map) and usinfo_map['city'] != '':\n",
    "        sql_city = '''\n",
    "            select\n",
    "                a.user_id,\n",
    "                b.city\n",
    "            from\n",
    "                (select user_id from samples_tmp) as a\n",
    "            left join\n",
    "                (select\n",
    "                    {0} as user_id,\n",
    "                    {1} as city\n",
    "                from {2}\n",
    "                where {3} = '{4}' ) as b\n",
    "            on a.user_id = b.user_id\n",
    "\n",
    "        '''.format(usinfo_map['user_id'], usinfo_map['city'], user_table, usinfo_map['dt'], cur_str)\n",
    "        city_df = spark.sql(sql_city).toPandas()\n",
    "        city_df.fillna(value=0, inplace=True)\n",
    "        city_df[['user_id', 'city']] = city_df[['user_id', 'city']].astype(str)\n",
    "        if userlabel_df.empty:\n",
    "            userlabel_df = city_df\n",
    "        else:\n",
    "            userlabel_df = pd.merge(userlabel_df, city_df, how='inner', on='user_id')\n",
    "        userlabel_df['city'] = le.fit_transform(userlabel_df['city'])\n",
    "\n",
    "    if ('consume_levle' in usinfo_map) and usinfo_map['consume_level'] != '':\n",
    "        sql_consume_lvl = '''\n",
    "            select\n",
    "                a.user_id,\n",
    "                b.consume_lvl\n",
    "            from\n",
    "                (select user_id from samples_tmp) as a\n",
    "            left join\n",
    "                (select\n",
    "                    {0} as user_id,\n",
    "                    {1} as consume_lvl\n",
    "                from {2}\n",
    "                where {3} = '{4}' ) as b\n",
    "            on a.user_id = b.user_id\n",
    "\n",
    "        '''.format(usinfo_map['user_id'], usinfo_map['consume_level'], user_table, usinfo_map['dt'], cur_str)\n",
    "        consume_lvl_df = spark.sql(sql_consume_lvl).toPandas()\n",
    "        consume_lvl_df.fillna(value=0, inplace=True)\n",
    "        consume_lvl_df[['user_id', 'consume_lvl']] = consume_lvl_df[['user_id', 'consume_lvl']].astype(str)\n",
    "        if userlabel_df.empty:\n",
    "            userlabel_df = consume_lvl_df\n",
    "        else:\n",
    "            userlabel_df = pd.merge(userlabel_df, consume_lvl_df, how='inner', on='user_id')\n",
    "        userlabel_df['consume_lvl'] = le.fit_transform(userlabel_df['consume_lvl'])\n",
    "\n",
    "    if ('online_signup_time' in usinfo_map) and usinfo_map['online_signup_time'] != '':\n",
    "        sql_sign_on = '''\n",
    "            select\n",
    "                a.user_id,\n",
    "                b.sign_on_days\n",
    "            from\n",
    "                (select user_id from samples_tmp) as a\n",
    "            left join\n",
    "                (select\n",
    "                    {0} as user_id,\n",
    "                    if({1} is null, 0, round(datediff(from_unixtime(unix_timestamp(\"{2}\",\"yyyy-MM-dd\"),\"yyyy-MM-dd\"),from_unixtime(unix_timestamp(substr({1},1,10),\"yyyy-MM-dd\"),\"yyyy-MM-dd\")),0)) as sign_on_days\n",
    "                from {4}\n",
    "                where {3} = '{2}' ) as b\n",
    "            on a.user_id = b.user_id\n",
    "\n",
    "        '''.format(usinfo_map['user_id'], usinfo_map['online_signup_time'], cur_str, usinfo_map['dt'], user_table)\n",
    "        #         print(sql_sign_on)\n",
    "        sign_on_df = spark.sql(sql_sign_on).toPandas()\n",
    "        sign_on_df.fillna(value=0, inplace=True)\n",
    "        sign_on_df[['user_id']] = sign_on_df[['user_id']].astype(str)\n",
    "        sign_on_df[['sign_on_days']] = sign_on_df[['sign_on_days']].astype(np.int64)\n",
    "        if userlabel_df.empty:\n",
    "            userlabel_df = sign_on_df\n",
    "        else:\n",
    "            userlabel_df = pd.merge(userlabel_df, sign_on_df, how='inner', on='user_id')\n",
    "\n",
    "    if ('recent_view_day' in usinfo_map) and usinfo_map['recent_view_day'] != '':\n",
    "        sql_latest_view = '''\n",
    "            select\n",
    "                a.user_id,\n",
    "                b.latest_view_days\n",
    "            from\n",
    "                (select user_id from samples_tmp) as a\n",
    "            left join\n",
    "                (select\n",
    "                    {0} as user_id,\n",
    "                    if({1} is null, 0, round(datediff(from_unixtime(unix_timestamp(\"{2}\",\"yyyy-MM-dd\"),\"yyyy-MM-dd\"),from_unixtime(unix_timestamp(substr({1},1,10),\"yyyy-MM-dd\"),\"yyyy-MM-dd\")),0)) as latest_view_days\n",
    "                from {4}\n",
    "                where {3} = '{2}'\n",
    "               ) as b\n",
    "            on a.user_id = b.user_id\n",
    "\n",
    "        '''.format(usinfo_map['user_id'], usinfo_map['recent_view_day'], cur_str, usinfo_map['dt'], user_table)\n",
    "        latest_view_df = spark.sql(sql_latest_view).toPandas()\n",
    "        latest_view_df.fillna(value=0, inplace=True)\n",
    "        latest_view_df[['user_id']] = latest_view_df[['user_id']].astype(str)\n",
    "        latest_view_df[['latest_view_days']] = latest_view_df[['latest_view_days']].astype(np.int64)\n",
    "        if userlabel_df.empty:\n",
    "            userlabel_df = latest_view_df\n",
    "        else:\n",
    "            userlabel_df = pd.merge(userlabel_df, latest_view_df, how='inner', on='user_id')\n",
    "\n",
    "    # if  usinfo_map['是否线上新客']:\n",
    "    #     sql_consume_online = '''\n",
    "    #         select\n",
    "    #             a.user_id,\n",
    "    #             b.is_consume_online\n",
    "    #         from\n",
    "    #             (select user_id from samples_tmp) as a\n",
    "    #         left join\n",
    "    #             (select\n",
    "    #                 {0} as user_id,\n",
    "    #                 {1} as is_consume_online\n",
    "    #             from {2}\n",
    "    #             where {3} = {4} ) as b\n",
    "    #         on a.user_id = b.user_id\n",
    "    #         where b.user_id is not null\n",
    "    #     '''.format( usinfo_map['user_id'],  usinfo_map['是否线上新客'],  user_table,  usinfo_map['dt'],  cur_str)\n",
    "    #     consume_online_df =  spark.sql(sql_consume_online).toPandas()\n",
    "    #     consume_online_df[['user_id','is_consume_online']] =  consume_online_df[['user_id','is_consume_online']].astype(str)\n",
    "    #     if not userlabel_df:\n",
    "    #         userlabel_df = consume_online_df\n",
    "    #     else:\n",
    "    #         userlabel_df = pd.merge(userlabel_df, consume_online_df, how='inner', on='user_id')\n",
    "\n",
    "    if ('life_stage' in usinfo_map) and usinfo_map['life_stage'] != '':\n",
    "        sql_consume_online = '''\n",
    "            select\n",
    "                a.user_id,\n",
    "                b.life_stage\n",
    "            from\n",
    "                (select user_id from samples_tmp) as a\n",
    "            left join\n",
    "                (select\n",
    "                    {0} as user_id,\n",
    "                    {1} as life_stage\n",
    "                from {2}\n",
    "                where {3} = '{4}' ) as b\n",
    "            on a.user_id = b.user_id\n",
    "\n",
    "        '''.format(usinfo_map['user_id'], usinfo_map['life_stage'], user_table, usinfo_map['dt'], cur_str)\n",
    "        consume_online_df = spark.sql(sql_consume_online).toPandas()\n",
    "        consume_online_df.fillna(value=0, inplace=True)\n",
    "        consume_online_df[['user_id', 'life_stage']] = consume_online_df[['user_id', 'life_stage']].astype(str)\n",
    "        if userlabel_df.empty:\n",
    "            userlabel_df = consume_online_df\n",
    "        else:\n",
    "            userlabel_df = pd.merge(userlabel_df, consume_online_df, how='inner', on='user_id')\n",
    "        userlabel_df['life_stage'] = le.fit_transform(userlabel_df['life_stage'])\n",
    "\n",
    "    if ('is_new' in usinfo_map) and usinfo_map['is_new'] != '':\n",
    "        sql_consume_online = '''\n",
    "            select\n",
    "                a.user_id,\n",
    "                b.is_new\n",
    "            from\n",
    "                (select user_id from samples_tmp) as a\n",
    "            left join\n",
    "                (select\n",
    "                    {0} as user_id,\n",
    "                    {1} as is_new\n",
    "                from {2}\n",
    "                where {3} = '{4}' ) as b\n",
    "            on a.user_id = b.user_id\n",
    "\n",
    "        '''.format(usinfo_map['user_id'], usinfo_map['is_new'], user_table, usinfo_map['dt'], cur_str)\n",
    "        consume_online_df = spark.sql(sql_consume_online).toPandas()\n",
    "        consume_online_df.fillna(value=0, inplace=True)\n",
    "        consume_online_df[['user_id', 'is_new']] = consume_online_df[['user_id', 'is_new']].astype(str)\n",
    "        if userlabel_df.empty:\n",
    "            userlabel_df = consume_online_df\n",
    "        else:\n",
    "            userlabel_df = pd.merge(userlabel_df, consume_online_df, how='inner', on='user_id')\n",
    "        userlabel_df['is_new'] = le.fit_transform(userlabel_df['is_new'])\n",
    "\n",
    "    if ('is_consume_online' in usinfo_map) and usinfo_map['is_consume_online'] != '':\n",
    "        sql_consume_online = '''\n",
    "            select\n",
    "                a.user_id,\n",
    "                b.is_consume_online\n",
    "            from\n",
    "                (select user_id from samples_tmp) as a\n",
    "            left join\n",
    "                (select\n",
    "                    {0} as user_id,\n",
    "                    {1} as is_consume_online\n",
    "                from {2}\n",
    "                where {3} = '{4}' ) as b\n",
    "            on a.user_id = b.user_id\n",
    "\n",
    "        '''.format(usinfo_map['user_id'], usinfo_map['is_consume_online'], user_table, usinfo_map['dt'], cur_str)\n",
    "        consume_online_df = spark.sql(sql_consume_online).toPandas()\n",
    "        consume_online_df.fillna(value=0, inplace=True)\n",
    "        consume_online_df[['user_id', 'is_consume_online']] = consume_online_df[\n",
    "            ['user_id', 'is_consume_online']].astype(str)\n",
    "        if userlabel_df.empty:\n",
    "            userlabel_df = consume_online_df\n",
    "        else:\n",
    "            userlabel_df = pd.merge(userlabel_df, consume_online_df, how='inner', on='user_id')\n",
    "        userlabel_df['is_consume_online'] = le.fit_transform(userlabel_df['is_consume_online'])\n",
    "    #     print(userlabel_df)\n",
    "    return userlabel_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "input_params = {\"taskid\":124,\"category\":[\"蔬菜\",\"水果\"],\"userData\":{\"is_consume_online\":\"new_online_consume_flag\",\"is_new\":\"online_signup_flag\",\"city\":\"city_code\",\"sex\":\"sex_id\",\"consume_level\":\"\",\"yuliu_id\":\"\",\"tableName\":\"labelx.push_vip_id\",\"dt\":\"dt\",\"recent_view_day\":\"\",\"province\":\"\",\"user_id\":\"vip_id\",\"online_signup_time\":\"signup_date\",\"age\":\"age\"},\"trafficData\":{\"cart_remove\":\"\",\"cart_add\":\"\",\"click\":\"CLICK\",\"tableName\":\"labelx.push_push_traffic_retail_vipid\",\"duration\":\"duration\",\"search\":\"\",\"exposure\":\"EXPOSURE\",\"card_add\":\"CART_ADD\",\"user_id\":\"user_id\",\"event_code\":\"event_code\",\"sku\":\"sku\",\"collect\":\"COLLECT\",\"event_time\":\"event_time\",\"browse\":\"\"},\"orderData\":{\"user_id\":\"user_id\",\"order_time\":\"order_time\",\"sku\":\"sku\",\"sale_quantity\":\"sale_quantity\",\"order_id\":\"order_id\",\"sale_amount\":\"sale_amount\",\"tableName\":\"labelx.push_oder_retail\"},\"goodsData\":{\"dt\":\"dt\",\"cate\":\"cate\",\"sku\":\"sku\",\"tableName\":\"labelx.push_goods\"},\"trainingScope\":\"过去60天\",\"forecastPeriod\":\"未来30天\",\"eventCode\":{\"event_code\":{\"search\":\"\",\"cart_remove\":\"\",\"exposure\":\"EXPOSURE\",\"cart_add\":\"\",\"click\":\"CLICK\",\"collect\":\"COLLECT\",\"browse\":\"\"}}}\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o187.load.\n: com.starrocks.connector.spark.exception.ConnectedFailedException: Connect to http://172.21.32.16:8030/api/labelx/push_vip_id/_schemafailed, status code is 404.\n\tat com.starrocks.connector.spark.rest.RestService.send(RestService.java:151)\n\tat com.starrocks.connector.spark.rest.RestService.getSchema(RestService.java:227)\n\tat com.starrocks.connector.spark.sql.SchemaUtils$.discoverSchemaFromFe(SchemaUtils.scala:53)\n\tat com.starrocks.connector.spark.sql.SchemaUtils$.discoverSchema(SchemaUtils.scala:43)\n\tat com.starrocks.connector.spark.sql.StarrocksRelation.lazySchema$lzycompute(StarrocksRelation.scala:50)\n\tat com.starrocks.connector.spark.sql.StarrocksRelation.lazySchema(StarrocksRelation.scala:50)\n\tat com.starrocks.connector.spark.sql.StarrocksRelation.schema(StarrocksRelation.scala:54)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:417)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_28457/4043544841.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     21\u001B[0m                             \u001B[0minput_params\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'goodsData'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m                             \u001B[0minput_params\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'eventCode'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0minput_params\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'trafficData'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'event_code'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m                              None)\n\u001B[0m\u001B[1;32m     24\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_28457/1667987215.py\u001B[0m in \u001B[0;36mConstructFeatures\u001B[0;34m(self, train_period, predict_period, cat_list, is_train, order_table, bh_table, user_table, item_table, odinfo, bhinfo, usinfo, itinfo, bh_code_map, where_sql)\u001B[0m\n\u001B[1;32m     42\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mis_train\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m             cur_str = get_min_date_of_4_table(spark, user_table, order_table, item_table, bh_table, usinfo, odinfo,\n\u001B[0;32m---> 44\u001B[0;31m                                               itinfo, bhinfo)\n\u001B[0m\u001B[1;32m     45\u001B[0m             \u001B[0;31m# print(cur_str)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             cur_str = (datetime.datetime.strptime(cur_str, '%Y-%m-%d') - datetime.timedelta(days=30)).strftime(\n",
      "\u001B[0;32m/tmp/ipykernel_28457/1667987215.py\u001B[0m in \u001B[0;36mget_min_date_of_4_table\u001B[0;34m(spark, user_table, order_table, item_table, bh_table, usinfo, odinfo, itinfo, bhinfo)\u001B[0m\n\u001B[1;32m     92\u001B[0m     \u001B[0msql3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'''select max({0}) as it_max_date from {1} '''\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitinfo\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'dt'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mitem_table\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     93\u001B[0m     \u001B[0msql4\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'''select substr(max({0}),1,10) as bh_max_date from {1} '''\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbhinfo\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'event_time'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbh_table\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 94\u001B[0;31m     \u001B[0mspark_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muser_table\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'user_table_tmp'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     95\u001B[0m     \u001B[0mspark_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder_table\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'order_table_tmp'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     96\u001B[0m     \u001B[0mspark_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mitem_table\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'item_table_tmp'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_28457/1667987215.py\u001B[0m in \u001B[0;36mspark_read\u001B[0;34m(spark, table_name, tempViewName)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mspark_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtable_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtempViewName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m     df = spark.read.format('starrocks').option('starrocks.table.identifier', f'{table_name}').option(\n\u001B[0;32m---> 14\u001B[0;31m         'starrocks.fenodes', '172.21.32.16:8030').option('user', 'root').option('password', '').load()\n\u001B[0m\u001B[1;32m     15\u001B[0m     \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateOrReplaceTempView\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{tempViewName}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'success creating temp view: {tempViewName}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark-2.4.8-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    170\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 172\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    173\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0msince\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1.4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark-2.4.8-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1255\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1256\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1257\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1258\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1259\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark-2.4.8-bin-hadoop2.7/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     61\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark-2.4.8-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o187.load.\n: com.starrocks.connector.spark.exception.ConnectedFailedException: Connect to http://172.21.32.16:8030/api/labelx/push_vip_id/_schemafailed, status code is 404.\n\tat com.starrocks.connector.spark.rest.RestService.send(RestService.java:151)\n\tat com.starrocks.connector.spark.rest.RestService.getSchema(RestService.java:227)\n\tat com.starrocks.connector.spark.sql.SchemaUtils$.discoverSchemaFromFe(SchemaUtils.scala:53)\n\tat com.starrocks.connector.spark.sql.SchemaUtils$.discoverSchema(SchemaUtils.scala:43)\n\tat com.starrocks.connector.spark.sql.StarrocksRelation.lazySchema$lzycompute(StarrocksRelation.scala:50)\n\tat com.starrocks.connector.spark.sql.StarrocksRelation.lazySchema(StarrocksRelation.scala:50)\n\tat com.starrocks.connector.spark.sql.StarrocksRelation.schema(StarrocksRelation.scala:54)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:417)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "create_data = CreateDataset()\n",
    "cate_list = []\n",
    "for cateid in input_params['category']:\n",
    "    cate = '\"'+cateid+'\"'\n",
    "    cate_list.append(cate)\n",
    "catestr = \"(\"+\",\".join(cate_list)+\")\"\n",
    "\n",
    "\n",
    "dataset = create_data.ConstructFeatures(\n",
    "                            input_params['trainingScope'],\n",
    "                            input_params['forecastPeriod'],\n",
    "                            catestr,\n",
    "                            True,\n",
    "                            input_params['orderData']['tableName'],\n",
    "                            input_params['trafficData']['tableName'],\n",
    "                            input_params['userData']['tableName'],\n",
    "                            input_params['goodsData']['tableName'],\n",
    "                            input_params['orderData'],\n",
    "                            input_params['trafficData'],\n",
    "                            input_params['userData'],\n",
    "                            input_params['goodsData'],\n",
    "                            input_params['eventCode'][input_params['trafficData']['event_code']],\n",
    "                             None)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}